{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "duplicate-representative",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ITAM-DS/deep-learning/blob/master/codigo/08-VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "encouraging-reviewer",
   "metadata": {
    "id": "grave-stake"
   },
   "outputs": [],
   "source": [
    "# !pip install -U d2l\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-sapphire",
   "metadata": {},
   "source": [
    "## Aplicaci√≥n de Batch-Normalization al modelo LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accessible-craft",
   "metadata": {
    "id": "fiscal-vegetation"
   },
   "outputs": [],
   "source": [
    "def net():\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5,\n",
    "                               input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('sigmoid'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=5),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('sigmoid'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(120),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('sigmoid'),\n",
    "        tf.keras.layers.Dense(84),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('sigmoid'),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "introductory-reach",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lovely-samoa",
    "outputId": "f8210a3e-8b68-4f13-9cbe-41bd7306fd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D output shape:\t (1, 24, 24, 6)\n",
      "BatchNormalization output shape:\t (1, 24, 24, 6)\n",
      "Activation output shape:\t (1, 24, 24, 6)\n",
      "MaxPooling2D output shape:\t (1, 12, 12, 6)\n",
      "Conv2D output shape:\t (1, 8, 8, 16)\n",
      "BatchNormalization output shape:\t (1, 8, 8, 16)\n",
      "Activation output shape:\t (1, 8, 8, 16)\n",
      "MaxPooling2D output shape:\t (1, 4, 4, 16)\n",
      "Flatten output shape:\t (1, 256)\n",
      "Dense output shape:\t (1, 120)\n",
      "BatchNormalization output shape:\t (1, 120)\n",
      "Activation output shape:\t (1, 120)\n",
      "Dense output shape:\t (1, 84)\n",
      "BatchNormalization output shape:\t (1, 84)\n",
      "Activation output shape:\t (1, 84)\n",
      "Dense output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform(shape=(1, 28, 28, 1))\n",
    "for layer in net().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welsh-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2D Parameters:  156 \t. Output shape:\t (1, 24, 24, 6)\n",
      "BatchNormalization Parameters:  24 \t. Output shape:\t (1, 24, 24, 6)\n",
      "Activation Parameters:  0.0 \t. Output shape:\t (1, 24, 24, 6)\n",
      "MaxPooling2D Parameters:  0.0 \t. Output shape:\t (1, 12, 12, 6)\n",
      "Conv2D Parameters:  2416 \t. Output shape:\t (1, 8, 8, 16)\n",
      "BatchNormalization Parameters:  64 \t. Output shape:\t (1, 8, 8, 16)\n",
      "Activation Parameters:  0.0 \t. Output shape:\t (1, 8, 8, 16)\n",
      "MaxPooling2D Parameters:  0.0 \t. Output shape:\t (1, 4, 4, 16)\n",
      "Flatten Parameters:  0.0 \t. Output shape:\t (1, 256)\n",
      "Dense Parameters:  30840 \t. Output shape:\t (1, 120)\n",
      "BatchNormalization Parameters:  480 \t. Output shape:\t (1, 120)\n",
      "Activation Parameters:  0.0 \t. Output shape:\t (1, 120)\n",
      "Dense Parameters:  10164 \t. Output shape:\t (1, 84)\n",
      "BatchNormalization Parameters:  336 \t. Output shape:\t (1, 84)\n",
      "Activation Parameters:  0.0 \t. Output shape:\t (1, 84)\n",
      "Dense Parameters:  850 \t. Output shape:\t (1, 10)\n",
      "------------------------------------------------------------\n",
      "Total parameters:  45330.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform((1, 28, 28, 1))\n",
    "parameters = 0\n",
    "\n",
    "for layer in net().layers:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, \"Parameters: \",  \n",
    "          np.sum([np.prod(p.shape) for p in layer.get_weights()]), \n",
    "          '\\t. Output shape:\\t', X.shape)\n",
    "    parameters += np.sum([np.prod(p.shape) for p in layer.get_weights()])\n",
    "\n",
    "print(\"--\"*30)\n",
    "print(\"Total parameters: \", parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accessory-baker",
   "metadata": {
    "id": "recovered-latino"
   },
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=28)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-traffic",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Tomados del [libro de texto](https://d2l.ai/chapter_convolutional-modern/batch-norm.html):\n",
    "\n",
    "```{markdown}\n",
    "    1. Can we remove the bias parameter from the fully-connected layer or the convolutional layer before the batch normalization? Why?\n",
    "    2. Compare the learning rates for LeNet with and without batch normalization.\n",
    "        - Plot the increase in training and test accuracy.\n",
    "        - How large can you make the learning rate?\n",
    "    3. Do we need batch normalization in every layer? Experiment with it?\n",
    "    4. Can you replace dropout by batch normalization? How does the behavior change?\n",
    "    5. Fix the parameters beta and gamma, and observe and analyze the results.\n",
    "    6. Review the online documentation for BatchNorm from the high-level APIs to see the other applications for batch normalization.\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "08-VGG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
